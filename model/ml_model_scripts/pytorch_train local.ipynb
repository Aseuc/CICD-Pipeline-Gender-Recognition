{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importieren der benötigten Bibliotheken für das ML-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, cuda\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, cuda\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train_men = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train/men\"\n",
    "train_women = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train/women\"\n",
    "val_men = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val/men\"\n",
    "val_women = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val/women\"\n",
    "csv = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/source_csv/list_attr_celeba.csv\"\n",
    "folder = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/img_align_celeba\"\n",
    "train = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train\"\n",
    "test = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val\"\n",
    "def process_images(csv, folder, train_men, train_women, test_men, test_women, num_train, num_test):\n",
    "    # Lese die CSV-Datei\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    # Extrahiere die 'Male'-Spalte\n",
    "    male = df['Male']\n",
    "\n",
    "    # Erstelle Listen für Männer- und Frauenbilder\n",
    "    men_images = df[df['Male'] == 1]['image_id'].tolist()\n",
    "    women_images = df[df['Male'] == -1]['image_id'].tolist()\n",
    "    \n",
    "    # men_features = [item for sublist in df[df['Male'] == 1].values.tolist()[:num_train] for item in sublist]\n",
    "    # women_features = [item for sublist in df[df['Male'] == -1].values.tolist()[:num_train] for item in sublist]\n",
    "    men_train_features = df[df['Male'] == 1][:num_train]\n",
    "    women_train_features = df[df['Male'] == -1][:num_train]\n",
    "\n",
    "    men_train_features.to_csv( train+ '/men_features.csv', index=False)\n",
    "    women_train_features.to_csv( train+ '/women_features.csv', index=False)\n",
    "\n",
    "  \n",
    "    men_test_features = df[df['Male'] == 1][num_train:num_train+num_test]\n",
    "    women_test_features = df[df['Male'] == -1][num_train:num_train+num_test]\n",
    "\n",
    "    men_test_features.to_csv( test+ '/men_features.csv', index=False)\n",
    "    women_test_features.to_csv( test+ '/women_features.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "    # Kopiere die ausgewählten Bilder in die Trainingsordner\n",
    "    for img in men_images[:num_train]:\n",
    "        shutil.copy(img, train_men)\n",
    "    for img in women_images[:num_train]:\n",
    "         shutil.copy(img, train_women)\n",
    "\n",
    "    # Kopiere die ausgewählten Bilder in die Testordner\n",
    "    for img in men_images[num_train:num_train+num_test]:\n",
    "               shutil.copy(img, test_men)\n",
    "    for img in women_images[num_train:num_train+num_test]:\n",
    "        shutil.copy(img, test_women)\n",
    "\n",
    "# Verwendung der Funktion\n",
    "process_images(csv=csv, folder=folder, train_men=train_men,train_women=train_women,test_men=val_men,test_women=val_women,num_train=2000,num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def process_images(source_csv, source_img_folder, output_folder_train_men, output_folder_train_women, num_images):\n",
    "    # Lese die CSV-Datei\n",
    "    df = pd.read_csv(source_csv)\n",
    "\n",
    "    # Extrahiere die 'Male'-Spalte\n",
    "    male = df['Male']\n",
    "\n",
    "    # Erstelle Listen für Männer- und Frauenbilder\n",
    "    men_images = df[df['Male'] == 1]['image_id'].tolist()[:num_images]\n",
    "    women_images = df[df['Male'] == -1]['image_id'].tolist()[:num_images]\n",
    "  \n",
    "     # Kopiere die restlichen Bilder in die Validierungsordner, wenn sie nicht bereits in den Trainingsordnern vorhanden sind\n",
    "    for img in men_images[num_images:]:\n",
    "        if not os.path.exists(os.path.join(output_folder_train_men, img)):\n",
    "            shutil.copy(img, output_folder_val_men)\n",
    "    for img in women_images[num_images:]:\n",
    "        if not os.path.exists(os.path.join(output_folder_train_women, img)):\n",
    "            shutil.copy(img, output_folder_val_women)\n",
    "\n",
    "# Verwendung der Funktion\n",
    "process_images('C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/source_csv/list_attr_celeba.csv', 'C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/img_align_celeba', 'C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output/train/men','C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output/train/women', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML-Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Festlegen der epochen und der Batchsize sowie die Transformierung der Daten in einen Tensor und dann noch normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "def count_files(men_dir, women_dir):\n",
    "    men_files = len(os.listdir(men_dir))\n",
    "    women_files = len(os.listdir(women_dir))\n",
    "\n",
    "    total_files = men_files + women_files\n",
    "\n",
    "    print(f\"Anzahl der Dateien in 'men': {men_files}\")\n",
    "    print(f\"Anzahl der Dateien in 'women': {women_files}\")\n",
    "    print(f\"Gesamtanzahl der Dateien: {total_files}\")\n",
    "\n",
    "\n",
    "# Transformation der Daten für das Training und Testen  \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((178, 218)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Dateipfade für das Training und Testen festlegen\n",
    "train_dataset = datasets.ImageFolder(root='C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train',transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root= 'C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val',transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Erstellen eines eigenen Datasets (falls notwendig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "class GenderRecognitionDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 1])\n",
    "        image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[idx, 2]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return (image,y_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Überprüfen ob die Bilder richtig angezeigt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "img = train_features[0].permute(1, 2, 0)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Erstellen eines CNN-Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das CNN-Modell besteht aus 2 Convolutional Layers, 2 Pooling Layers, 2 Max Pooling Layers, 3 Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Erste Convolutional Layer. Nimmt 3 Eingangskanäle (RGB), gibt 6 Kanäle aus, mit einer Kernelgröße von 5\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  \n",
    "        # Max-Pooling-Layer mit einem quadratischen Fenster der Kernelgröße=4, Schrittgröße=4\n",
    "        self.pool = nn.MaxPool2d(2, 2)  \n",
    "        # Zweite Convolutional Layer. Nimmt 6 Eingangskanäle (von der vorherigen Schicht), gibt 16 Kanäle aus, mit einer Kernelgröße von 5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Max-Pooling-Layer mit einem quadratischen Fenster der Kernelgröße=2, Schrittgröße=2\n",
    "        self.pool = nn.MaxPool2d(2,2) \n",
    "        # Erste vollständig verbundene Schicht. Nimmt einen abgeflachten Vektor der Größe 33456 auf, gibt einen Vektor der Größe 120 aus\n",
    "        self.fc1 = nn.Linear(33456 , 120) \n",
    "        # Zweite vollständig verbundene Schicht. Nimmt einen Vektor der Größe 120 auf, gibt einen Vektor der Größe 84 aus\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Dritte vollständig verbundene Schicht. Nimmt einen Vektor der Größe 84 auf, gibt einen Vektor der Größe 2 aus\n",
    "        self.fc3 = nn.Linear(84, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Anwendung der ersten Conv-Schicht, dann ReLU-Aktivierungsfunktion, dann Max-Pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Anwendung der zweiten Conv-Schicht, dann ReLU-Aktivierungsfunktion, dann Max-Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Abflachen des Tensorausgangs von den Conv-Schichten\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Anwendung der ersten vollständig verbundenen Schicht, dann ReLU-Aktivierungsfunktion\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        # Anwendung der zweiten vollständig verbundenen Schicht, dann ReLU-Aktivierungsfunktion\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        # Anwendung der dritten vollständig verbundenen Schicht\n",
    "        x = self.fc3(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML-Modell trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dokumentation für PyTorch Trainingsskript\n",
    "\n",
    "Dieses Skript trainiert ein Convolutional Neural Network (CNN) mit PyTorch.\n",
    "\n",
    "## Variablen\n",
    "\n",
    "- `model`: Eine Instanz des `SimpleCNN` Modells.\n",
    "- `criterion`: Die Verlustfunktion, die während des Trainings verwendet wird. In diesem Fall wird die Cross-Entropy-Loss-Funktion verwendet.\n",
    "- `optimizer`: Der Optimierer, der zur Aktualisierung der Modellparameter verwendet wird. Hier wird der Stochastic Gradient Descent (SGD) Optimierer verwendet.\n",
    "- `patience`: Die Anzahl der Epochen, die auf eine Verbesserung der Genauigkeit gewartet wird, bevor das Training gestoppt wird.\n",
    "- `best_accuracy`: Die beste Genauigkeit, die während des Trainings erreicht wurde. Initialisiert auf 0.\n",
    "- `early_stopping_counter`: Zählt die Anzahl der Epochen ohne Verbesserung der Genauigkeit.\n",
    "\n",
    "## Trainingsschleife\n",
    "\n",
    "Das Modell wird für eine bestimmte Anzahl von Epochen trainiert. In jeder Epoche wird das Modell mit den Trainingsdaten trainiert und dann mit den Testdaten validiert.\n",
    "\n",
    "Während des Trainings werden die Modellparameter aktualisiert, um den Verlust zu minimieren. Der Verlust wird berechnet, indem die Ausgabe des Modells und die tatsächlichen Labels verglichen werden.\n",
    "\n",
    "Nach jeder Epoche wird die Genauigkeit des Modells auf den Testdaten berechnet. Wenn die Genauigkeit über 90% liegt, wird der aktuelle Zustand des Modells gespeichert. Wenn die Genauigkeit nicht besser ist als die bisher beste Genauigkeit, wird der `early_stopping_counter` erhöht. Wenn der `early_stopping_counter` den Wert von `patience` erreicht, wird das Training gestoppt.\n",
    "\n",
    "## Ausgabe\n",
    "\n",
    "Das Skript gibt den Verlust und die Genauigkeit nach jeder Epoche aus. Wenn das Training aufgrund von Early Stopping gestoppt wird, wird eine entsprechende Nachricht ausgegeben. Am Ende des Trainings wird eine Nachricht ausgegeben, dass das Training abgeschlossen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "patience = 10 \n",
    "best_accuracy = 0.0  \n",
    "early_stopping_counter = 0  \n",
    "\n",
    "for epoch in range(epochs): \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(train_dataloader), 0):\n",
    "      \n",
    "        inputs, labels = data\n",
    "\n",
    "   \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "   \n",
    "        running_loss += loss.item()\n",
    "\n",
    "  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if i % 10 == 9: \n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, running_loss / 100))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for val_data in test_dataloader:\n",
    "            val_images, val_labels = val_data\n",
    "            val_outputs = model(val_images)\n",
    "            _, predicted = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "\n",
    "\n",
    "    if accuracy > 0.9:  \n",
    "        torch.save(model.state_dict(), f'model/PyTorch_Trained_Models/model_epoch_{epoch+1}_accuracy_{accuracy:.2f}.pth')\n",
    "   \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        early_stopping_counter = 0\n",
    "        print(f\"Genauigkeit: {accuracy:.2f}\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "model_path_git = f'model/PyTorch_Trained_Models/'\n",
    "model_test_path = f'test/model_to_be_tested'\n",
    "\n",
    "torch.save(model.state_dict(), f'{model_path_git}model_git{batch_size}' + '-' + f'{epochs}' + '.pth')\n",
    "torch.save(model.state_dict(), f'{model_test_path}' + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 ML-Model Testen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 PyTorch Modell Inferenz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Modell Inferenz\n",
    "\n",
    "Dieser Codeblock führt Inferenz (Vorhersagen) auf einem Testdatensatz mit einem vortrainierten PyTorch Modell durch.\n",
    "\n",
    "##### Modellpfad und Gerätekonfiguration\n",
    "\n",
    "```python\n",
    "model_path = f'{model_path_git}model_git{batch_size}' + '-' + f'{epochs}' + '.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "Zuerst wird der Pfad zum vortrainierten Modell erstellt, indem der Basispfad, der Name des Modells, die Batch-Größe und die Anzahl der Epochen kombiniert werden. Dann wird das Gerät auf \"cuda\" gesetzt, wenn eine GPU verfügbar ist, sonst auf \"cpu\".\n",
    "\n",
    "##### Modellinitialisierung und Gewichtsladung\n",
    "\n",
    "```python\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "```\n",
    "\n",
    "Ein neues Modell wird erstellt (in diesem Fall ein einfaches CNN). Das Modell wird dann mit den Gewichten geladen, die unter dem angegebenen Pfad gespeichert sind.\n",
    "\n",
    "##### Inferenz auf dem Testdatensatz\n",
    "\n",
    "```python\n",
    "for inputs, _ in test_dataloader:\n",
    "    inputs = inputs.to(device) \n",
    "    output = model(inputs)\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    predictions = torch.argmax(probabilities, dim=1)\n",
    "    predictions_list = predictions.cpu().numpy().tolist()\n",
    "    print(predictions_list)\n",
    "```\n",
    "\n",
    "Für jede Eingabe im Testdatensatz werden die Eingaben auf das Gerät verschoben (GPU oder CPU). Das Modell macht dann eine Vorhersage auf den Eingaben. Die Ausgabe des Modells wird in Wahrscheinlichkeiten umgewandelt, indem die Softmax-Funktion angewendet wird. Die Klasse mit der höchsten Wahrscheinlichkeit wird als Vorhersage ausgewählt. Schließlich werden die Vorhersagen in eine Liste umgewandelt und ausgegeben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Pfad zum Modell wird erstellt, indem der Basispfad, der Name des Modells, die Batch-Größe und die Anzahl der Epochen kombiniert werden.\n",
    "model_path = f'{model_path_git}model_git{batch_size}' + '-' + f'{epochs}' + '.pth'\n",
    "\n",
    "# Das Gerät wird auf \"cuda\" gesetzt, wenn eine GPU verfügbar ist, sonst auf \"cpu\".\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ein neues Modell wird erstellt (in diesem Fall ein einfaches CNN).\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Das Modell wird mit den Gewichten geladen, die unter dem angegebenen Pfad gespeichert sind.\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Ein DataLoader für den Testdatensatz wird erstellt (diese Zeile ist auskommentiert).\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Für jede Eingabe im Testdatensatz:\n",
    "for inputs, _ in test_dataloader:\n",
    "    # Die Eingaben werden auf das Gerät verschoben (GPU oder CPU).\n",
    "    inputs = inputs.to(device) \n",
    "    # Das Modell macht eine Vorhersage auf den Eingaben.\n",
    "    output = model(inputs)\n",
    "    # Die Ausgabe des Modells wird in Wahrscheinlichkeiten umgewandelt, indem die Softmax-Funktion angewendet wird.\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # Die Klasse mit der höchsten Wahrscheinlichkeit wird als Vorhersage ausgewählt.\n",
    "    predictions = torch.argmax(probabilities, dim=1)\n",
    "    # Die Vorhersagen werden in eine Liste umgewandelt und ausgegeben.\n",
    "    predictions_list = predictions.cpu().numpy().tolist()\n",
    "    print(predictions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PyTorch Modell Inferenz und Genauigkeitsberechnung\n",
    "\n",
    "Dieser Codeblock führt Inferenz (Vorhersagen) auf einem Testdatensatz mit einem vortrainierten PyTorch Modell durch und berechnet die Genauigkeit der Vorhersagen.\n",
    "\n",
    "### Modellladen und Testdaten-Loader\n",
    "\n",
    "```python\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "```\n",
    "\n",
    "Zuerst wird ein neues Modell erstellt (in diesem Fall ein einfaches CNN) und mit den Gewichten geladen, die unter dem angegebenen Pfad gespeichert sind. Dann wird ein DataLoader für den Testdatensatz erstellt, mit einer Batch-Größe von 64 und ohne Shuffle.\n",
    "\n",
    "### Inferenz und Genauigkeitsberechnung\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in test_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print('Genauigkeit des Modells auf Testbilder: {}%'.format(100 * accuracy))\n",
    "```\n",
    "\n",
    "Zwei Zähler werden initialisiert: `correct` für die Anzahl der korrekten Vorhersagen und `total` für die Gesamtzahl der Vorhersagen. Für jede Eingabe und das zugehörige Label im Testdatensatz werden die Eingaben und Labels auf das Gerät verschoben (GPU oder CPU). Das Modell macht dann eine Vorhersage auf den Eingaben und die Klasse mit der höchsten Ausgabe wird als Vorhersage ausgewählt. Die `total` und `correct` Zähler werden entsprechend aktualisiert. Schließlich wird die Genauigkeit berechnet als das Verhältnis von `correct` zu `total` und ausgegeben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model laden\n",
    "model = SimpleCNN()\n",
    "# model.load_state_dict(torch.load(f'model/model{batch_size}' + '-' + f'{epochs}' + '.pth'))\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# Assume `test_dataset` is your ImageFolder dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model für die Predictions und Genauigkeitberechnung verwenden\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for inputs, labels in test_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)    \n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy = correct / total\n",
    "print('Genauigkeit des Modells auf Testbilder: {}%'.format(100 * accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.3 PyTorch Modell Inferenz und Metrikenberechnung\n",
    "\n",
    "Dieser Codeblock führt Inferenz (Vorhersagen) auf einem Testdatensatz mit einem vortrainierten PyTorch Modell durch und berechnet verschiedene Metriken zur Beurteilung der Modellleistung.\n",
    "\n",
    "### Modell Evaluierung und Inferenz\n",
    "\n",
    "```python\n",
    "model.eval() \n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted)\n",
    "        true_labels.extend(labels)\n",
    "```\n",
    "\n",
    "Zuerst wird das Modell in den Evaluierungsmodus gesetzt. Dann werden zwei Listen initialisiert: `predictions` für die Vorhersagen des Modells und `true_labels` für die tatsächlichen Labels. Für jede Eingabe und das zugehörige Label im Testdatensatz macht das Modell eine Vorhersage und die Klasse mit der höchsten Ausgabe wird als Vorhersage ausgewählt. Die Vorhersagen und tatsächlichen Labels werden den entsprechenden Listen hinzugefügt.\n",
    "\n",
    "### Konvertierung in numpy-Arrays und Metrikenberechnung\n",
    "\n",
    "```python\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f'Genauigkeit: {accuracy}, Präzision: {precision}, Recall: {recall}, F1-Score: {f1}')\n",
    "```\n",
    "\n",
    "Die Listen `predictions` und `true_labels` werden in numpy-Arrays umgewandelt. Dann werden verschiedene Metriken berechnet: Genauigkeit, gewichtete Präzision, gewichteter Recall und gewichteter F1-Score. Diese Metriken werden ausgegeben.\n",
    "\n",
    "### Speichern der Metriken\n",
    "\n",
    "```python\n",
    "with open('model/metrics/metrics.txt', 'w') as outfile:\n",
    "    outfile.write(f'Modellmetriken: Genauigkeit: {accuracy}, Präzision: {precision}, Recall: {recall}, F1-Score: {f1}')\n",
    "```\n",
    "\n",
    "Die berechneten Metriken werden in einer Textdatei gespeichert.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze das Modell in den Evaluierungsmodus\n",
    "model.eval() \n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "\n",
    "# Konvertiere die Listen in numpy-Arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Berechne die Metriken\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f'Genauigkeit: {accuracy}, Präzision: {precision}, Recall: {recall}, F1-Score: {f1}')\n",
    "\n",
    "with open('model/metrics/metrics.txt', 'w') as outfile:\n",
    "    outfile.write(f'Modellmetriken: Genauigkeit: {accuracy}, Präzision: {precision}, Recall: {recall}, F1-Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Flatten the outputs and convert to numpy array\n",
    "        predictions.extend(outputs.view(-1).cpu().numpy())\n",
    "        labels_list.extend(labels.cpu().numpy())\n",
    "        \n",
    "# Flatten the arrays\n",
    "predictions = np.array(predictions).ravel()\n",
    "labels_list = np.array(labels_list).ravel()\n",
    "print(predictions)\n",
    "print(labels_list)\n",
    "\n",
    "# Ensure both arrays have the same length\n",
    "min_length = min(len(predictions), len(labels_list))\n",
    "predictions = predictions[:min_length]\n",
    "labels_list = labels_list[:min_length]\n",
    "\n",
    "predictions = np.array(predictions).ravel()\n",
    "labels_list = np.array(labels_list).ravel()\n",
    "\n",
    "# Plot true labels against predictions\n",
    "# plt.scatter(labels_list,predictions)\n",
    "plt.scatter(labels_list,predictions)\n",
    "plt.grid(True)\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('True Labels',color='blue')\n",
    "plt.ylabel('Predictions',color='red')\n",
    "plt.title('True Labels vs Predictions')\n",
    "\n",
    "plt.savefig(\"model/plots/plot_scatter.jpg\",dpi=100)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot([labels_list.min(), labels_list.max()], [predictions.min(), predictions.max()], 'k--', lw=4)\n",
    "plt.grid(True)\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predictions')\n",
    "plt.xlabel('True Labels',color='blue')\n",
    "plt.ylabel('Predictions',color='red')\n",
    "plt.title('True Labels vs Predictions')\n",
    "plt.savefig(\"model/plots/plot_plt.jpg\",dpi=100)\n",
    "plt.show()\n",
    "\n",
    "# Create a 2D histogram from the data\n",
    "heatmap_data, xedges, yedges = np.histogram2d(labels_list, predictions, bins=50)\n",
    "\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.imshow(heatmap_data, origin='lower', cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Anzahl')\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title('Heatmap of True Labels vs Predictions')\n",
    "plt.savefig(\"model/plots/heatmap.jpg\", dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 ML-Test mit Fairlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 Daten für Fairlearn vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Erste Convolutional Layer. Nimmt 3 Eingangskanäle (RGB), gibt 6 Kanäle aus, mit einer Kernelgröße von 5\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  \n",
    "        # Max-Pooling-Layer mit einem quadratischen Fenster der Kernelgröße=4, Schrittgröße=4\n",
    "        self.pool = nn.MaxPool2d(2, 2)  \n",
    "        # Zweite Convolutional Layer. Nimmt 6 Eingangskanäle (von der vorherigen Schicht), gibt 16 Kanäle aus, mit einer Kernelgröße von 5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Max-Pooling-Layer mit einem quadratischen Fenster der Kernelgröße=2, Schrittgröße=2\n",
    "        self.pool = nn.MaxPool2d(2,2) \n",
    "        # Erste vollständig verbundene Schicht. Nimmt einen abgeflachten Vektor der Größe 33456 auf, gibt einen Vektor der Größe 120 aus\n",
    "        self.fc1 = nn.Linear(33456 , 120) \n",
    "        # Zweite vollständig verbundene Schicht. Nimmt einen Vektor der Größe 120 auf, gibt einen Vektor der Größe 84 aus\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Dritte vollständig verbundene Schicht. Nimmt einen Vektor der Größe 84 auf, gibt einen Vektor der Größe 2 aus\n",
    "        self.fc3 = nn.Linear(84, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Anwendung der ersten Conv-Schicht, dann ReLU-Aktivierungsfunktion, dann Max-Pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Anwendung der zweiten Conv-Schicht, dann ReLU-Aktivierungsfunktion, dann Max-Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Abflachen des Tensorausgangs von den Conv-Schichten\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Anwendung der ersten vollständig verbundenen Schicht, dann ReLU-Aktivierungsfunktion\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        # Anwendung der zweiten vollständig verbundenen Schicht, dann ReLU-Aktivierungsfunktion\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        # Anwendung der dritten vollständig verbundenen Schicht\n",
    "        x = self.fc3(x)  \n",
    "        return x\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "y_pred = []\n",
    "sensitive_features = [\"men\",\"women\"]\n",
    "batch_size = 64\n",
    "train_men = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train/men\"\n",
    "train_women = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train/women\"\n",
    "val_men = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val/men\"\n",
    "val_women = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val/women\"\n",
    "csv = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/source_csv/list_attr_celeba.csv\"\n",
    "folder = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/img_align_celeba\"\n",
    "train = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/train\"\n",
    "test = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/output2/val\"\n",
    "merged_csv = \"C:/Users/busse/Bachelorarbeit/CICD-Pipeline-Gender-Recognition/data/merged_features.csv\"   \n",
    "# Transformation der Daten für das Training und Testen  \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((178, 218)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.ImageFolder(train, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(test, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)     \n",
    "# Lesen Sie die CSV-Datei\n",
    "df = pd.read_csv(merged_csv)\n",
    "\n",
    "# Extrahieren Sie die Werte der 'male' Spalte\n",
    "sensitive_features = df['Male'].tolist()\n",
    "sensitive_features = pd.Series(sensitive_features).replace({-1: 'Frau', 1: 'Mann'}).tolist()\n",
    "\n",
    "\n",
    "\n",
    "model_path = r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\model\\PyTorch_Trained_Models\\model64-1.pth\"\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "for inputs, labels in train_dataloader:\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    y_test.extend(labels.numpy())\n",
    "    y_pred.extend(preds.numpy())\n",
    "    # Fügen Sie hier Ihren Code hinzu, um die sensiblen Merkmale für jede Vorhersage zu extrahieren\n",
    "    # sensitive_features.extend(extracted_features)\n",
    "\n",
    "# Berechnen Sie Fairness-Metriken\n",
    "metrics = MetricFrame(accuracy_score, y_test, y_pred, sensitive_features=sensitive_features)\n",
    "\n",
    "# Drucken Sie die Fairness-Metriken aus\n",
    "print(metrics.by_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fairlearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairlearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_adult\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fairlearn'"
     ]
    }
   ],
   "source": [
    "# Importieren Sie die benötigte Bibliothek\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Erstellen Sie eine Liste der Gruppennamen und ihrer Genauigkeiten\n",
    "groups = metrics.by_group.index.tolist()\n",
    "accuracies = metrics.by_group.values.tolist()\n",
    "\n",
    "# Erstellen Sie ein Balkendiagramm\n",
    "plt.bar(groups, accuracies)\n",
    "\n",
    "# Fügen Sie Titel und Beschriftungen hinzu\n",
    "plt.title('Accuracy by group')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Zeigen Sie das Diagramm an\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    count,\n",
    "    false_negative_rate,\n",
    "    false_positive_rate,\n",
    "    selection_rate,\n",
    ")\n",
    "\n",
    "sensitive_features = pd.Series(sensitive_features).replace({-1: 'Frau', 1: 'Mann'}).tolist()\n",
    "# Analyze metrics using MetricFrame\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"false positive rate\": false_positive_rate,\n",
    "    \"false negative rate\": false_negative_rate,\n",
    "    \"selection rate\": selection_rate,\n",
    "    \"count\": count,\n",
    "}\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features\n",
    ")\n",
    "ax = metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Metriken anzeigen!\",\n",
    ")\n",
    "# Durchlaufen Sie jeden Subplot und passen Sie die y-Achsen-Ticks an\n",
    "for row in ax:\n",
    "    for subplot in row:\n",
    "        subplot.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "# Zeigen Sie die Plots an\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Customize plots with ylim\n",
    "metric_frame.by_group.plot(\n",
    "    kind=\"bar\",\n",
    "    ylim=[0, 2],\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    title=\"Show all metrics with assigned y-axis range\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize plots with colormap\n",
    "metric_frame.by_group.plot(\n",
    "    kind=\"bar\",\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    colormap=\"Accent\",\n",
    "    title=\"Show all metrics in Accent colormap\",\n",
    ")\n",
    "\n",
    "\n",
    "# Saving plots\n",
    "fig = metric_frame.by_group[[\"count\"]].plot(\n",
    "    kind=\"pie\",\n",
    "    subplots=True,\n",
    "    layout=[1, 1],\n",
    "    legend=True,\n",
    "    figsize=[12, 8],\n",
    "    labels=[\"Frau\",\"Mann\"],\n",
    "    autopct=\"%.2f\",\n",
    "    title=\"Metriken als Kuchen-Diagramm\",\n",
    ")\n",
    "\n",
    "# Don't save file during doc build\n",
    "if \"__file__\" in locals():\n",
    "    fig[0][0].figure.savefig(\"filename.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Erklärbarkeit des CNN-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "# Laden Sie Ihr Modell\n",
    "model = SimpleCNN()\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='conv1')\n",
    "gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='pool')\n",
    "gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='conv2')\n",
    "gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='fc1')\n",
    "gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='fc2')\n",
    "gradcam = GradCAM.from_config(model_type='resnet', arch=model, layer_name='fc3')\n",
    "\n",
    "# Angenommen, `dataloader` ist Ihr DataLoader\n",
    "for input_batch, target_batch in train_dataloader:\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    # Führen Sie das Modell aus und erhalten Sie die Ausgabe\n",
    "    output = model(input_batch)\n",
    "\n",
    "    # Berechnen Sie die GradCAM-Maske\n",
    "    mask, _ = gradcam(input_batch)\n",
    "\n",
    "    # Visualisieren Sie die GradCAM-Maske\n",
    "    heatmap, result = visualize_cam(mask, input_batch)\n",
    "\n",
    "    # Zeigen Sie das Bild und die GradCAM-Maske an\n",
    "    plt.imshow(np.transpose(input_batch[0].cpu().numpy(), (1, 2, 0)))\n",
    "    plt.imshow(np.transpose(result[0].cpu().numpy(), (1, 2, 0)), alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    break  # Nur der erste Batch wird verarbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from torchvision.models import resnet50\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from model import SimpleCNN\n",
    "import torch\n",
    "\n",
    "\n",
    "# Definieren Sie die Transformationen, die auf die Bilder angewendet werden sollen\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((178, 218)),  # Ändern Sie die Größe auf die von Ihrem Modell erwartete Größe\n",
    "    transforms.ToTensor(),  # Konvertiert das Bild in einen Tensor und skaliert die Werte auf [0, 1]\n",
    "])\n",
    "path = r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\data\\output2\\train\"\n",
    "model_path = r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\model\\PyTorch_Trained_Models\\model_epoch_9_accuracy_0.92.pth\"\n",
    "\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))) \n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere die Bibliotheken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torch.autograd import Variable\n",
    "# Definiere die Architektur des Modells\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    # Convolutional part\n",
    "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "    self.pool1 = nn.MaxPool2d(2, 2) # output: 32x89x109\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # output: 64x89x109\n",
    "    self.pool2 = nn.MaxPool2d(2, 2) # output: 64x44x54\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # output: 128x44x54\n",
    "    self.pool3 = nn.MaxPool2d(2, 2) # output: 128x22x27\n",
    "    self.relu = nn.ReLU()\n",
    "    # Linear part\n",
    "    self.fc1 = nn.Linear(128*22*27, 256) # output: 256\n",
    "    # self.fc1 = nn.Linear(100352, 256)\n",
    "    self.fc2 = nn.Linear(256, 32)\n",
    "    # self.fc2 = nn.Linear(100352, 40)\n",
    "    self.sigmoid = nn.Sigmoid() # <- add the sigmoid function\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Convolutional part\n",
    "    x = self.pool1(self.relu(self.conv1(x)))\n",
    "    x = self.pool2(self.relu(self.conv2(x)))\n",
    "    x = self.pool3(self.relu(self.conv3(x)))\n",
    "    # Flatten the output\n",
    "    x = torch.flatten(x, 1)\n",
    "    # Linear part\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.sigmoid(self.fc2(x)) # <- apply the sigmoid function\n",
    "    return x\n",
    "\n",
    "# Erstelle ein Modell-Objekt\n",
    "model = CNN()\n",
    "\n",
    "# Erstelle ein Dataset-Objekt\n",
    "transform = transforms.Compose([transforms.Resize((178, 218)), transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Ändere den folgenden Code, um ImageFolder zu verwenden\n",
    "# trainset = torchvision.datasets.CelebA(root='./data', split='train', download=True, transform=transform)\n",
    "# testset = torchvision.datasets.CelebA(root='./data', split='test', download=True, transform=transform)\n",
    "trainset = torchvision.datasets.ImageFolder(root=r'C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\data\\output2\\train', transform=transform) # <- use ImageFolder for trainset\n",
    "testset = torchvision.datasets.ImageFolder(root=r'C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\data\\output2\\val', transform=transform) # <- use ImageFolder for testset\n",
    "\n",
    "# Erstelle einen DataLoader-Objekt\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Definiere den Verlust und den Optimierer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Trainiere das Modell\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    # Hole die Eingaben, die Labels und die Pfade\n",
    "    inputs, labels = data # <- use only two variables\n",
    "    labels = labels[:40] # <- remove the colon from the index\n",
    "    paths = trainset.imgs[i][0] # <- use the imgs attribute to get the path\n",
    "    b_x = Variable(inputs) # batch x (image)\n",
    "    b_y = Variable(labels).float() # batch y (target)\n",
    "    output = model(b_x)[0].float()\n",
    "    loss = criterion(output, b_y).float()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Test -> this is where I have no clue\n",
    "   \n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model4_params.pt\")\n",
    "torch.save(model, \"model4_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcam.methods import SmoothGradCAMpp\n",
    "import torch\n",
    "model.load_state_dict(torch.load(\"model2_params.pt\"))\n",
    "\n",
    "\n",
    "# Wende torch grad-cam an\n",
    "# Wähle ein Bild aus dem Testset\n",
    "img, label = testset[0] # <- use only two variables\n",
    "path = testset.imgs[0][0] # <- use the imgs attribute to get the path\n",
    "# Erstelle einen CAM-Extraktor\n",
    "model = model.requires_grad_(False)\n",
    "with SmoothGradCAMpp(model) as cam_extractor:\n",
    "  # Mache eine Vorhersage\n",
    "  out = model(img.unsqueeze(0))\n",
    "  # Erhalte die Aktivierungskarte\n",
    "  activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "# Zeige das Bild und die Aktivierungskarte an\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img.squeeze(0), cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(activation_map, cmap='jet')\n",
    "plt.title('Activation Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "\n",
    "# Lade dein Modell\n",
    "model = CNN()\n",
    "model.load_state_dict(torch.load(\"model3_params.pt\"))\n",
    "\n",
    "# Gib die letzte Faltungsschicht an\n",
    "target_layer = model.conv3\n",
    "\n",
    "# Setze requires_grad auf True für das Modell\n",
    "model.requires_grad_(False)\n",
    "\n",
    "# Definiere deine Eingabe\n",
    "img, label = testset[3]\n",
    "\n",
    "# Setze requires_grad auf True für die Eingabe\n",
    "img.requires_grad_(True)\n",
    "\n",
    "# Erstelle einen CAM-Extraktor mit SmoothGradCAMpp\n",
    "with SmoothGradCAMpp(model, target_layer) as cam_extractor:\n",
    "  # Mache eine Vorhersage mit deinem Modell\n",
    "  out = model(img.unsqueeze(0)).long()\n",
    "  # Berechne die Aktivierungskarte mit dem CAM-Extraktor\n",
    "  activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "# Ersetze torch.tensor durch torch.stack, um deine Liste von Tensoren in einen Tensor zu konvertieren\n",
    "activation_map = torch.stack(activation_map)\n",
    "\n",
    "# Plotte die Aktivierungskarte mit dem Originalbild\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "img_permuted = img.permute(1, 2, 0)\n",
    "plt.imshow(img_permuted.detach().numpy())\n",
    "plt.title(f'Label: {label}')\n",
    "plt.subplot(1, 2, 2)\n",
    "# Überprüfen Sie die Anzahl der Dimensionen von activation_map\n",
    "# if activation_map.dim() == 2:\n",
    "#     activation_map = activation_map.permute(1,2,0)\n",
    "# elif activation_map.dim() == 3:\n",
    "#     activation_map = activation_map.permute(0,0,0)\n",
    "# else:\n",
    "#     print(\"Unexpected number of dimensions in activation_map\")\n",
    "\n",
    "activation_map_squeezed = activation_map.squeeze()\n",
    "plt.imshow(activation_map.squeeze(), cmap='jet')\n",
    "plt.title('Activation Map')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchcam.methods import GradCAM\n",
    "from PIL import Image\n",
    "# Lade dein trainiertes Modell und wähle die letzte Faltungsschicht aus\n",
    "model = models.resnet18(pretrained=True) # oder dein eigenes Modell\n",
    "target_layer = model.layer4[-1].conv2 # oder eine andere Schicht\n",
    "\n",
    "# Erstelle einen CAM-Extraktor mit der GradCAM Klasse\n",
    "cam_extractor = GradCAM(model, target_layer)\n",
    "\n",
    "# Definiere deine Eingabe und mache eine Vorhersage mit deinem Modell\n",
    "img, label = testset[0] # oder eine andere Eingabe\n",
    "out = model(img.unsqueeze(0))\n",
    "\n",
    "# Berechne die Aktivierungskarte mit dem CAM-Extraktor\n",
    "activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "# Überlagere die Aktivierungskarte mit dem Originalbild\n",
    "from torchcam.utils import overlay_mask\n",
    "result = overlay_mask(img, activation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren Sie die benötigten Bibliotheken\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchcam.methods import GradCAM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchcam.methods import SmoothGradCAMpp, LayerCAM, XGradCAM, ScoreCAM,GradCAMpp\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    # Convolutional part\n",
    "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "    self.pool1 = nn.MaxPool2d(2, 2) # output: 32x89x109\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # output: 64x89x109\n",
    "    self.pool2 = nn.MaxPool2d(2, 2) # output: 64x44x54\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1) # output: 128x44x54\n",
    "    self.pool3 = nn.MaxPool2d(2, 2) # output: 128x22x27\n",
    "    self.relu = nn.ReLU()\n",
    "    # Linear part\n",
    "    self.fc1 = nn.Linear(128*22*27, 256) # output: 256\n",
    "    # self.fc1 = nn.Linear(100352, 256)\n",
    "    self.fc2 = nn.Linear(256, 32)\n",
    "    # self.fc2 = nn.Linear(100352, 40)\n",
    "    self.sigmoid = nn.Sigmoid() # <- add the sigmoid function\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Convolutional part\n",
    "    x = self.pool1(self.relu(self.conv1(x)))\n",
    "    x = self.pool2(self.relu(self.conv2(x)))\n",
    "    x = self.pool3(self.relu(self.conv3(x)))\n",
    "    # Flatten the output\n",
    "    x = torch.flatten(x, 1)\n",
    "    # Linear part\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.sigmoid(self.fc2(x)) # <- apply the sigmoid function\n",
    "    return x\n",
    "  \n",
    "\n",
    "#model = CNN()\n",
    "\n",
    "\n",
    "model_path = r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\model\\PyTorch_Trained_Models\\model64-91acc-100.pth\"\n",
    "# Definieren oder laden Sie Ihr eigenes PyTorch-Modell\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "# Erstellen Sie einen CAM-Extraktor mit Ihrem Modell und der gewünschten Schicht\n",
    "# Zum Beispiel ein Grad-CAM-Extraktor für die dritte Faltungsschicht\n",
    "cam_extractor = GradCAMpp(model, \"conv2\")\n",
    "\n",
    "# Lesen Sie Ihr Eingabebild ein und verarbeiten Sie es für Ihr Modell\n",
    "# Zum Beispiel ein Bild von einer Katze\n",
    "img = torchvision.io.read_image(r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\data\\output2\\train\\men\\000008.jpg\")\n",
    "input_tensor = torchvision.transforms.functional.normalize(\n",
    "    torchvision.transforms.functional.resize(img, (178, 218)) / 255.,\n",
    "    [0.485, 0.456, 0.406],\n",
    "    [0.220, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "# Führen Sie Ihr Modell mit dem Eingabebild aus\n",
    "out = model(input_tensor.unsqueeze(0))\n",
    "\n",
    "# Rufen Sie den CAM-Extraktor mit dem Klassenindex und dem Modellausgang auf\n",
    "# Zum Beispiel der Index für die Klasse \"Katze\"\n",
    "activation_map = cam_extractor(1, out)\n",
    "\n",
    "# Konvertieren Sie die Aktivierungskarte in ein numpy ndarray\n",
    "activation_map = activation_map[0].squeeze(0).numpy()\n",
    "\n",
    "# Visualisieren Sie die Aktivierungskarte\n",
    "plt.imshow(activation_map, cmap='jet')\n",
    "plt.axis('on')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pfad zum Ordner\n",
    "folder_path1 = r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\data\\output2\\train\\men\"\n",
    "folder_path2 = r\"C:\\Users\\busse\\Bachelorarbeit\\CICD-Pipeline-Gender-Recognition\\data\\output2\\train\\women\"\n",
    "\n",
    "\n",
    "def activation_map_vis(folder_path,label=\"Bild\"):\n",
    "    # Liste der .jpg-Dateien im Ordner erstellen\n",
    "    \n",
    "    jpg_files = [f for f in os.listdir(folder_path) if f.endswith(\".jpg\")]\n",
    "\n",
    "    # Über die ersten 5 .jpg-Dateien iterieren\n",
    "    for i in range(min(5, len(jpg_files))):\n",
    "        img_path = os.path.join(folder_path, jpg_files[i])\n",
    "        \n",
    "        # Lesen Sie Ihr Eingabebild ein und verarbeiten Sie es für Ihr Modell\n",
    "        img = torchvision.io.read_image(img_path)\n",
    "        input_tensor = torchvision.transforms.functional.normalize(\n",
    "            torchvision.transforms.functional.resize(img, (178, 218)) / 255.,\n",
    "            [0.485, 0.456, 0.406],\n",
    "            [0.220, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        # Führen Sie Ihr Modell mit dem Eingabebild aus\n",
    "        out = model(input_tensor.unsqueeze(0))\n",
    "\n",
    "        # Rufen Sie den CAM-Extraktor mit dem Klassenindex und dem Modellausgang auf\n",
    "        activation_map = cam_extractor(1, out)\n",
    "\n",
    "        # Konvertieren Sie die Aktivierungskarte in ein numpy ndarray\n",
    "        activation_map = activation_map[0].squeeze(0).numpy()\n",
    "        # Skalieren Sie die Aktivierungskarte auf die Größe des Originalbildes\n",
    "        activation_map_resized = cv2.resize(activation_map, (img.shape[2], img.shape[1]))\n",
    "\n",
    "        # Visualisieren Sie das Originalbild\n",
    "        plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "        # Visualisieren Sie die skalierte Aktivierungskarte\n",
    "        # plt.imshow(activation_map_resized, cmap='jet', alpha=0.5)\n",
    "\n",
    "        # Visualisieren Sie die Aktivierungskarte\n",
    "        plt.title(label)\n",
    "        # plt.imshow(activation_map, cmap='jet')\n",
    "        plt.imshow(activation_map_resized, cmap='jet', alpha=0.5)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "activation_map_vis(folder_path1, label=\"Mann Bild\")\n",
    "activation_map_vis(folder_path2, label=\"Frau Bild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
